{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y9PnAH1lVUtx"
      },
      "source": [
        "##### Copyright \u0026copy; 2020 Google Inc.\n",
        "\n",
        "\u003cfont size=-1\u003eLicensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.\u003c/font\u003e\n",
        "\u003chr/\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4ny1E3iZVUtz"
      },
      "source": [
        "# Managed Pipelines EAP: Create custom functions and containers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tsSgITuVVUtz"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[AI Platform Pipelines - Managed (Managed Pipelines)](https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#) makes it easier for you to run your ML Pipelines in a scalable and cost-effective way, while offering you ‘no lock-in’ flexibility. You build your pipelines in Python using [TensorFlow Extended (TFX)](tensorflow.org/tfx), and then execute your pipelines on Google Cloud serverlessly. You don’t have to worry about scale and only pay for what you use. (You can also take the same TFX pipelines and run them using Kubeflow Pipelines).\n",
        "\n",
        "This notebook shows examples of how to create custom AI Platform Pipelines components.  If you find that you need to build your own components, there are three ways to do this : \n",
        "\n",
        "1. You can use the [TFX Custom Components SDK](https://www.tensorflow.org/tfx/guide/custom_component)\n",
        "1. You can convert any Python function into a TFX Custom Component \n",
        "1. You can take any container and use that as a component. \n",
        "\n",
        "This notebook walks through examples for the 2nd and 3rd options. It also shows an example of how you can specify a pipeline with task-based dependencies.\n",
        "\n",
        "\n",
        "The notebook is designed to run on AI Platform Notebooks. If you want to run this notebook in your own development environment, you will need to do a bit more setup first.  See [these instructions](\u003chttps://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#heading=h.pyk4nfqsszzz\u003e).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BI_SB5cgVUt2"
      },
      "source": [
        "## Step 1: Follow the 'before you begin' steps in the Managed Pipelines User Guide\n",
        "\n",
        "Before proceeeding, make sure that you've followed all the steps in the [\"Before you Begin\" section](https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#heading=h.65kbhyyf93x0) of the Managed Pipelines User Guide.  You'll need to use the API key that you created for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u43xvtUaVUt2"
      },
      "source": [
        "## Step 2: set up your environment\n",
        "\n",
        "First, ensure that Python 3 is being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5mLQVedyVUt3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kcj5Q0ycVUt7"
      },
      "source": [
        "### Install the TFX SDK\n",
        "\n",
        "Next, we'll upgrade pip and install the TFX SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CIUcP-a8VUt8"
      },
      "outputs": [],
      "source": [
        "SDK_LOCATION = 'gs://caip-pipelines-sdk/releases/20200727/tfx-0.22.0.caip20200727-py3-none-any.whl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lDfDwLdgVUuC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pip --upgrade\n",
        "!gsutil cp {SDK_LOCATION} /tmp/tfx-0.22.0.caip20200727-py3-none-any.whl \u0026\u0026 pip install --no-cache-dir /tmp/tfx-0.22.0.caip20200727-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2IkhdU97VUuH"
      },
      "source": [
        "Next, install Skaffold. \n",
        "\n",
        "\u003e Note: if you're running this notebook in a non-linux local development environment, see [these Skaffold installation instructions](https://skaffold.dev/docs/install/) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_p1A3mpyVUuI"
      },
      "outputs": [],
      "source": [
        "# Install skaffold.\n",
        "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 \u0026\u0026 chmod +x skaffold \u0026\u0026 mkdir -p /home/jupyter/.local/bin \u0026\u0026 mv skaffold /home/jupyter/.local/bin/\n",
        "\n",
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rvvAKiYGVUuP"
      },
      "source": [
        "Ensure that you can import TFX and that its version is \u003e= 0.22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4LfOhelcVUuP"
      },
      "outputs": [],
      "source": [
        "# Check version\n",
        "import tfx\n",
        "tfx.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XMHmAO26VUuV"
      },
      "source": [
        "### Identify or Create a GCS bucket to use for your pipeline\n",
        "\n",
        "Below, you will need to specify a Google Gloud Storage (GCS) bucket for the Pipelines run to use.  If you do not already have one that you want to use, you can [create a new bucket](https://cloud.google.com/storage/docs/creating-buckets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rPTPNy0iVUuW"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Let's set up some variables used to customize the pipelines below. **Before you execute the following cell, make the indicated 'Change this' edits**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wnwe2_bDVUuX"
      },
      "outputs": [],
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "    \n",
        "USER = 'YOUR_USERNAME'  # Change this to your username.\n",
        "BUCKET_NAME = 'YOUR_GCS_BUCKET'  # Change this to your GCS bucket name.  Do not include the `gs://`.\n",
        "\n",
        "# It is not necessary to append your username to the pipeline root, \n",
        "# but this may be useful if multiple people are using the same project.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID' # Change this to your project id\n",
        "BASE_IMAGE = 'gcr.io/caip-pipelines-assets/tfx:0.22.0.caip20200727'\n",
        "\n",
        "API_KEY = 'YOUR_API_KEY'  # Change this to the API key that you created during initial setup\n",
        "# ENDPOINT = 'alpha-ml.googleapis.com'  # this is the default during EAP\n",
        "\n",
        "PIPELINE_ROOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WhUDXrmpVUub"
      },
      "source": [
        "## Step 3: Custom Python functions\n",
        "\n",
        "In this section, we will create components from dummy Python functions (that don't do much).\n",
        "\n",
        "We begin by writing a dummy 'preprocess' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j2LNuHOoVUub"
      },
      "outputs": [],
      "source": [
        "%%writefile my_preprocess.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf  # Used for writing files.\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "\n",
        "@component\n",
        "def MyPreprocess(training_data: OutputArtifact[Dataset]):\n",
        "  with tf.io.gfile.GFile(os.path.join(training_data.uri, 'training_data_file.txt'), 'w') as f:\n",
        "    f.write('Dummy training data')\n",
        "    \n",
        "  # We'll modify metadata and ensure that it gets passed to downstream components.  \n",
        "  training_data.set_string_custom_property('my_custom_field', 'my_custom_value')\n",
        "  training_data.set_string_custom_property('uri_for_output', training_data.uri)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ynW6qhXVUug"
      },
      "source": [
        "Next, we'll write a second component that uses the training data produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "M1JJspcRVUuh"
      },
      "outputs": [],
      "source": [
        "%%writefile my_trainer.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter\n",
        "\n",
        "\n",
        "@component\n",
        "def MyTraining(training_data: InputArtifact[Dataset],\n",
        "               model: OutputArtifact[Model],\n",
        "               num_iterations: Parameter[int] = 100):\n",
        "  # Let's read the contents of training data and write to the metadata.\n",
        "  with tf.io.gfile.GFile(\n",
        "      os.path.join(training_data.uri, 'training_data_file.txt'), 'r') as f:\n",
        "    contents = f.read()\n",
        "    model.set_string_custom_property('contents_of_training_data', contents)\n",
        "    model.set_int_custom_property('num_iterations_used', num_iterations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "21__INHzVUuk"
      },
      "source": [
        "Then, we'll write a finalizer component that collects all metadata, and dumps it. Ensure that `PIPELINE_ROOT` was set properly above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cXkd4CsLVUuk"
      },
      "outputs": [],
      "source": [
        "collector_template = f\"\"\"\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "from google.protobuf import json_format\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import *\n",
        "from tfx.utils import json_utils\n",
        "\n",
        "OUTPUT_LOCATION = '{PIPELINE_ROOT}/python_function_pipeline/metadata.json'\n",
        "\n",
        "@component\n",
        "def MetadataCollector(training_data: InputArtifact[Dataset],\n",
        "                      model: InputArtifact[Model]):\n",
        "  artifacts = [\n",
        "      json_format.MessageToDict(x)\n",
        "      for x in [training_data.mlmd_artifact, model.mlmd_artifact]\n",
        "  ]\n",
        "  with tf.io.gfile.GFile(OUTPUT_LOCATION, 'w') as f:\n",
        "    f.write(json.dumps(artifacts, indent=4))\n",
        "\"\"\"\n",
        "with open('metadata_collector.py', 'w') as f:\n",
        "    f.write(collector_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4KCoSe3VUuv"
      },
      "source": [
        "Next, let's package the above into a container.   \n",
        "In future, it will be possible to do this via the TFX CLI. For now, we'll do this using a Dockerfile and Skaffold. \n",
        "\n",
        "\u003e Note: If you're running this notebook on AI Platform Notebooks, Docker will be installed.  If you're running the notebook in a local development environment, you'll need to have Docker installed there. Confirm that you have [installed Skaffold](https://skaffold.dev/docs/install/) locally as well.\n",
        "\n",
        "First, we'll define a `skaffold.yaml` file.  We'll first define a string to use in creating the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wRz7udPaVUuw"
      },
      "outputs": [],
      "source": [
        "SK_TEMPLATE = \"{{{{.IMAGE_NAME}}}}:{}\".format(USER)\n",
        "print(SK_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6h2CR9MUVUuy"
      },
      "source": [
        "Now we'll write out the Skaffold yaml file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QkXyPriaVUuz"
      },
      "outputs": [],
      "source": [
        "skaffold_template = f\"\"\"\n",
        "apiVersion: skaffold/v2beta3\n",
        "kind: Config\n",
        "metadata:\n",
        "  name: my-pipeline\n",
        "build:\n",
        "  artifacts:\n",
        "  - image: 'gcr.io/{PROJECT_ID}/caip-tfx-custom'\n",
        "    context: .\n",
        "    docker:\n",
        "      dockerfile: Dockerfile\n",
        "  tagPolicy:\n",
        "    envTemplate:\n",
        "      template: \"{{SK_TEMPLATE}}\"\n",
        "\"\"\"\n",
        "with open('skaffold.yaml', 'w') as f:\n",
        "    f.write(skaffold_template.format(**globals()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3O63tfpaVUu3"
      },
      "source": [
        "Next, we'll define the `Dockerfile`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qE8TDTPTVUu3"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "FROM gcr.io/caip-pipelines-assets/tfx:latest\n",
        "WORKDIR /pipeline\n",
        "COPY ./ ./\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lLf3lqHhVUu8"
      },
      "source": [
        "Next, we'll build the container image. Below, we'll use it as the `default_image` when we run the pipeline.\n",
        "\n",
        "\u003e Note: if you get a permissions error running the build, try first running\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "in the notebook terminal window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oPwDxVUKVUu9"
      },
      "outputs": [],
      "source": [
        "!skaffold build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SuKbux-WVUvB"
      },
      "source": [
        "Next, let's specify a pipeline that uses the components we just defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bmBh4VJaVUvB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Only required for local run.\n",
        "from tfx.orchestration.metadata import sqlite_metadata_connection_config\n",
        "\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.orchestration.ai_platform_pipelines import ai_platform_pipelines_dag_runner\n",
        "\n",
        "from my_preprocess import MyPreprocess\n",
        "from my_trainer import MyTraining\n",
        "from metadata_collector import MetadataCollector\n",
        "\n",
        "PIPELINE_NAME = \"function-based-pipeline-{}\".format(USER)\n",
        "\n",
        "def function_based_pipeline(pipeline_root):\n",
        "    preprocess = MyPreprocess()\n",
        "\n",
        "    training = MyTraining(\n",
        "        training_data=preprocess.outputs['training_data'],\n",
        "        num_iterations=10000)\n",
        "    \n",
        "    collect = MetadataCollector(\n",
        "        training_data=preprocess.outputs['training_data'],\n",
        "        model=training.outputs['model'])\n",
        "    \n",
        "    pipeline_name = \"function-based-pipeline-{}\".format(USER)\n",
        "    return Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        # Only needed for local runs.\n",
        "        metadata_connection_config=sqlite_metadata_connection_config('metadata.sqlite'),\n",
        "        components=[preprocess, training, collect])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "np_SMgl4VUvG"
      },
      "source": [
        "Let's make sure this pipeline works locally, using the Beam runner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_j2rqj3iVUvG"
      },
      "outputs": [],
      "source": [
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "BeamDagRunner().run(function_based_pipeline(pipeline_root='/tmp/pipeline_root'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GzkKIasMVUvK"
      },
      "source": [
        "Check that the metadata was produced locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LzGf0xH8VUvL"
      },
      "outputs": [],
      "source": [
        "from ml_metadata import metadata_store\n",
        "from ml_metadata.proto import metadata_store_pb2\n",
        "\n",
        "connection_config = metadata_store_pb2.ConnectionConfig()\n",
        "connection_config.sqlite.filename_uri = 'metadata.sqlite'\n",
        "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
        "store = metadata_store.MetadataStore(connection_config)\n",
        "store.get_artifacts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EfZI5iZmVUvO"
      },
      "source": [
        "Now that we confirmed by the local run that things are working, let's run this pipeline with Managed Pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iVqa8KwnVUvP"
      },
      "outputs": [],
      "source": [
        "config = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunnerConfig(\n",
        "    project_id=PROJECT_ID,\n",
        "    display_name='function-based-pipeline-{}'.format(USER),\n",
        "    default_image='gcr.io/{}/caip-tfx-custom:{}'.format(PROJECT_ID, USER))\n",
        "\n",
        "runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(config=config)\n",
        "\n",
        "# If you want to inspect the pipeline proto, run the following and look at the file contents.\n",
        "# runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(config=config, output_filename='pipeline.json')\n",
        "# runner.compile(function_based_pipeline())\n",
        "\n",
        "runner.run(function_based_pipeline(pipeline_root=os.path.join(PIPELINE_ROOT, PIPELINE_NAME)), api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qq821nDLVUvS"
      },
      "source": [
        "When the pipeline is complete, we can check the final output file to see the metadata produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ziqsrh8CVUvT"
      },
      "outputs": [],
      "source": [
        "MD_URI = 'gs://{}/pipeline_root/{}/python_function_pipeline/metadata.json'.format(BUCKET_NAME, USER)\n",
        "MD_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8D_uSFZ5VUvZ"
      },
      "outputs": [],
      "source": [
        "!gsutil cat {MD_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xOGS8IPhVUvd"
      },
      "source": [
        "## Step 4: Custom containers\n",
        "\n",
        "In this section, we will build custom containers and chain them together as a pipeline.\n",
        "\n",
        "Again, no real ML is taking place, but this illustrates how we can pass data (using URIs) to custom containers. We will write functions that generate examples and then train using examples of that format, and build a new container for each. Then we'll use them to create a container-based pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VALyJPp2VUve"
      },
      "source": [
        "### Container 1: Generate examples\n",
        "\n",
        "First, we'll define and write out the `generate_examples.py` code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BjwfG6ZnVUvi"
      },
      "outputs": [],
      "source": [
        "%%writefile generate_examples.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def _serialize_example(example, label):\n",
        "  example_value = tf.io.serialize_tensor(example).numpy()\n",
        "  label_value = tf.io.serialize_tensor(label).numpy()\n",
        "  feature = {\n",
        "      'examples':\n",
        "          tf.train.Feature(\n",
        "              bytes_list=tf.train.BytesList(value=[example_value])),\n",
        "      'labels':\n",
        "          tf.train.Feature(bytes_list=tf.train.BytesList(value=[label_value])),\n",
        "  }\n",
        "  return tf.train.Example(features=tf.train.Features(\n",
        "      feature=feature)).SerializeToString()\n",
        "\n",
        "\n",
        "def _tf_serialize_example(example, label):\n",
        "  serialized_tensor = tf.py_function(_serialize_example, (example, label),\n",
        "                                     tf.string)\n",
        "  return tf.reshape(serialized_tensor, ())\n",
        "\n",
        "\n",
        "def generate_examples(training_data_uri, test_data_uri, config_file_uri):\n",
        "  (train_data, test_data), info = tfds.load(\n",
        "      # Use the version pre-encoded with an ~8k vocabulary.\n",
        "      'imdb_reviews/subwords8k',\n",
        "      # Return the train/test datasets as a tuple.\n",
        "      split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "      # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "      as_supervised=True,\n",
        "      with_info=True)\n",
        "\n",
        "  serialized_train_examples = train_data.map(_tf_serialize_example)\n",
        "  serialized_test_examples = test_data.map(_tf_serialize_example)\n",
        "\n",
        "  filename = os.path.join(training_data_uri, \"train.tfrecord\")\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(serialized_train_examples)\n",
        "\n",
        "  filename = os.path.join(test_data_uri, \"test.tfrecord\")\n",
        "  writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "  writer.write(serialized_test_examples)\n",
        "\n",
        "  encoder = info.features['text'].encoder\n",
        "  config = {\n",
        "      'vocab_size': encoder.vocab_size,\n",
        "  }\n",
        "  config_file = os.path.join(config_file_uri, \"config\")\n",
        "  with tf.io.gfile.GFile(config_file, 'w') as f:\n",
        "    f.write(json.dumps(config))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--training_data_uri', type=str)\n",
        "  parser.add_argument('--test_data_uri', type=str)\n",
        "  parser.add_argument('--config_file_uri', type=str)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "  generate_examples(args.training_data_uri, args.test_data_uri,\n",
        "                    args.config_file_uri)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iw2AO_HKVUvn"
      },
      "source": [
        "Next, we'll create a Dockerfile that builds a container to run `generate_examples.py`.  Note that we're also installing the `tensorflow_datasets` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bb2r72JRVUvo"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile.generate_examples\n",
        "\n",
        "FROM gcr.io/caip-pipelines-assets/tfx:latest\n",
        "WORKDIR /pipeline\n",
        "COPY generate_examples.py generate_examples.py\n",
        "RUN pip install tensorflow_datasets\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L62PAFnrVUv8"
      },
      "source": [
        "Then, we'll write out the Skaffold file and build the container image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xnrGES7LVUv9"
      },
      "outputs": [],
      "source": [
        "skaffold_template = f\"\"\"\n",
        "apiVersion: skaffold/v2beta3\n",
        "kind: Config\n",
        "metadata:\n",
        "  name: my-pipeline\n",
        "build:\n",
        "  artifacts:\n",
        "  - image: 'gcr.io/{PROJECT_ID}/caip-tfx-custom-container-generate'\n",
        "    context: .\n",
        "    docker:\n",
        "      dockerfile: Dockerfile.generate_examples\n",
        "  tagPolicy:\n",
        "    envTemplate:\n",
        "      template: \"{{SK_TEMPLATE}}\"\n",
        "\"\"\"\n",
        "with open('skaffold_generate_examples.yaml', 'w') as f:\n",
        "    f.write(skaffold_template.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "BwugG91EVUwI"
      },
      "outputs": [],
      "source": [
        "!skaffold build -f skaffold_generate_examples.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lpRLLFJkVUwS"
      },
      "source": [
        "### Container 2: Train Examples\n",
        "\n",
        "Next, we'll do the same for the 'Train Examples' custom container.  We'll first write out a `train_examples.py` file, then build a container that runs it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "11yDmyWjVUwU"
      },
      "outputs": [],
      "source": [
        "%%writefile train_examples.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _parse_example(record):\n",
        "  f = {\n",
        "      'examples': tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
        "      'labels': tf.io.FixedLenFeature((), tf.string, default_value='')\n",
        "  }\n",
        "  return tf.io.parse_single_example(record, f)\n",
        "\n",
        "\n",
        "def _to_tensor(record):\n",
        "  examples = tf.io.parse_tensor(record['examples'], tf.int64)\n",
        "  labels = tf.io.parse_tensor(record['labels'], tf.int64)\n",
        "  return (examples, labels)\n",
        "\n",
        "\n",
        "def train_examples(training_data_uri, test_data_uri, config_file_uri,\n",
        "                   output_model_uri, output_metrics_uri):\n",
        "  train_examples = tf.data.TFRecordDataset(\n",
        "      [os.path.join(training_data_uri, 'train.tfrecord')])\n",
        "  test_examples = tf.data.TFRecordDataset(\n",
        "      [os.path.join(test_data_uri, 'test.tfrecord')])\n",
        "\n",
        "  train_batches = train_examples.map(_parse_example).map(_to_tensor)\n",
        "  test_batches = test_examples.map(_parse_example).map(_to_tensor)\n",
        "\n",
        "  with tf.io.gfile.GFile(os.path.join(config_file_uri, 'config')) as f:\n",
        "    config = json.loads(f.read())\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(config['vocab_size'], 16),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  train_batches = train_batches.shuffle(1000).padded_batch(\n",
        "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
        "\n",
        "  test_batches = test_batches.padded_batch(\n",
        "      32, (tf.TensorShape([None]), tf.TensorShape([])))\n",
        "\n",
        "  history = model.fit(\n",
        "      train_batches,\n",
        "      epochs=10,\n",
        "      validation_data=test_batches,\n",
        "      validation_steps=30)\n",
        "\n",
        "  loss, accuracy = model.evaluate(test_batches)\n",
        "\n",
        "  metrics = {\n",
        "      'loss': str(loss),\n",
        "      'accuracy': str(accuracy),\n",
        "  }\n",
        "\n",
        "  model_json = model.to_json()\n",
        "  with tf.io.gfile.GFile(os.path.join(output_model_uri, 'model.json'),\n",
        "                         'w') as f:\n",
        "    f.write(model_json)\n",
        "\n",
        "  with tf.io.gfile.GFile(os.path.join(output_metrics_uri, 'metrics.json'),\n",
        "                         'w') as f:\n",
        "    f.write(json.dumps(metrics))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--training_data_uri', type=str)\n",
        "  parser.add_argument('--test_data_uri', type=str)\n",
        "  parser.add_argument('--config_file_uri', type=str)\n",
        "  parser.add_argument('--output_model_uri', type=str)\n",
        "  parser.add_argument('--output_metrics_uri', type=str)\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  train_examples(args.training_data_uri, args.test_data_uri,\n",
        "                 args.config_file_uri, args.output_model_uri,\n",
        "                 args.output_metrics_uri)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cIvOjNUVUwd"
      },
      "source": [
        "Next, we'll create a Dockerfile that builds a container to run `train_examples.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2Vj09O2oVUwg"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile.train_examples\n",
        "\n",
        "FROM gcr.io/caip-pipelines-assets/tfx:latest\n",
        "WORKDIR /pipeline\n",
        "COPY train_examples.py train_examples.py\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2zaN6ilKVUwr"
      },
      "source": [
        "Then, we'll write out the Skaffold file and build the container image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c5vrdpCGVUwr"
      },
      "outputs": [],
      "source": [
        "skaffold_template = f\"\"\"\n",
        "apiVersion: skaffold/v2beta3\n",
        "kind: Config\n",
        "metadata:\n",
        "  name: my-pipeline\n",
        "build:\n",
        "  artifacts:\n",
        "  - image: gcr.io/{PROJECT_ID}/caip-tfx-custom-container-train\n",
        "    context: .\n",
        "    docker:\n",
        "      dockerfile: Dockerfile.train_examples\n",
        "  tagPolicy:\n",
        "    envTemplate:\n",
        "      template: \"{{SK_TEMPLATE}}\"\n",
        "\"\"\"\n",
        "with open('skaffold_train_examples.yaml', 'w') as f:\n",
        "    f.write(skaffold_template.format(**globals()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TcKmwknjVUww"
      },
      "outputs": [],
      "source": [
        "!skaffold build -f skaffold_train_examples.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_TZIuOwGVUw5"
      },
      "source": [
        "### Define a container-based pipeline\n",
        "\n",
        "Now we're ready to define a pipeline that uses these containers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H1SYvLGIVUw5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.orchestration.ai_platform_pipelines import ai_platform_pipelines_dag_runner\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from tfx.types.experimental.simple_artifacts import File\n",
        "from tfx.types.experimental.simple_artifacts import Metrics\n",
        "from tfx.dsl.component.experimental.container_component import create_container_component\n",
        "from tfx.dsl.component.experimental.placeholders import InputUriPlaceholder\n",
        "from tfx.dsl.component.experimental.placeholders import OutputUriPlaceholder\n",
        "\n",
        "\n",
        "import absl\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "\n",
        "def container_based_pipeline():\n",
        "  generate = create_container_component(\n",
        "      name='GenerateExamples',\n",
        "      outputs={\n",
        "          'training_data': Dataset,\n",
        "          'test_data': Dataset,\n",
        "          'config_file': File,\n",
        "      },\n",
        "      image = 'gcr.io/{}/caip-tfx-custom-container-generate:{}'.format(PROJECT_ID, USER),\n",
        "      command=[\n",
        "        'python', '/pipeline/generate_examples.py',\n",
        "        '--training_data_uri', OutputUriPlaceholder('training_data'),\n",
        "        '--test_data_uri', OutputUriPlaceholder('test_data'),\n",
        "        '--config_file_uri', OutputUriPlaceholder('config_file'),\n",
        "      ])\n",
        "\n",
        "  train = create_container_component(\n",
        "      name='Train',\n",
        "      inputs={\n",
        "          'training_data': Dataset,\n",
        "          'test_data': Dataset,\n",
        "          'config_file': File,\n",
        "      },\n",
        "      outputs={\n",
        "          'model': Model,\n",
        "          'metrics': Metrics,\n",
        "      },\n",
        "      image='gcr.io/{}/caip-tfx-custom-container-train:{}'.format(PROJECT_ID, USER),\n",
        "      command=[\n",
        "        'python', '/pipeline/train_examples.py',\n",
        "        '--training_data_uri', InputUriPlaceholder('training_data'),\n",
        "        '--test_data_uri', InputUriPlaceholder('test_data'),\n",
        "        '--config_file_uri', InputUriPlaceholder('config_file'),\n",
        "        '--output_model_uri', OutputUriPlaceholder('model'),\n",
        "        '--output_metrics_uri', OutputUriPlaceholder('metrics'),\n",
        "      ])\n",
        "\n",
        "  generate_component = generate()\n",
        "  train_component = train(\n",
        "    training_data=generate_component.outputs['training_data'],\n",
        "    test_data=generate_component.outputs['test_data'],\n",
        "    config_file=generate_component.outputs['config_file'])\n",
        "\n",
        "  pipeline_name = \"container_based_pipeline_{}\".format(USER)\n",
        "  return Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        enable_cache=True,\n",
        "        pipeline_root=os.path.join(PIPELINE_ROOT, pipeline_name),\n",
        "        components=[generate_component, train_component])\n",
        "\n",
        "container_based_pipeline = container_based_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QXkEJtifVUw-"
      },
      "source": [
        "Now let's run the pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "d9v7KepWVUw_"
      },
      "outputs": [],
      "source": [
        "config = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunnerConfig(\n",
        "    project_id=PROJECT_ID,\n",
        "    display_name='container-based-pipeline-{}'.format(USER))\n",
        "\n",
        "runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(config=config)\n",
        "\n",
        "# If you want to inspect the pipeline proto, run the following and look at the file contents.\n",
        "# runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(config=config, output_filename='pipeline.json')\n",
        "# runner.compile(container_based_pipeline())\n",
        "\n",
        "runner.run(container_based_pipeline, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-VGfYFGUVUxJ"
      },
      "source": [
        "##  Step 5: Specifying Task-based dependencies\n",
        "\n",
        "In this section, we will run two steps of a pipeline using task-based dependencies (rather than I/O dependencies) to schedule them. We'll build and use the same container for both steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ifV_GaqEVUxK"
      },
      "outputs": [],
      "source": [
        "%%writefile task_based_step.py\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx.types.experimental.simple_artifacts import File\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact, Parameter\n",
        "\n",
        "\n",
        "@component\n",
        "def MyTaskBasedStep(\n",
        "    output_file: OutputArtifact[File], \n",
        "    step_number: Parameter[int] = 0, \n",
        "    contents: Parameter[str] = ''):\n",
        "  # Write out whatever string was passed in to the file.\n",
        "  \n",
        "  with tf.io.gfile.GFile(os.path.join(output_file.uri, 'output.txt'), 'w') as f:\n",
        "    f.write('Step {}: Contents: {}'.format(step_number, contents))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K6ESmr0VVUxS"
      },
      "source": [
        "Write out the Dockerfile..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sV8b5sb3VUxS"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile.task_based\n",
        "\n",
        "FROM gcr.io/caip-pipelines-assets/tfx:latest\n",
        "WORKDIR /pipeline\n",
        "COPY task_based_step.py task_based_step.py\n",
        "ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ipT0uKV2VUxX"
      },
      "source": [
        "...then write out the Skaffold file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PtIQqHOTVUxX"
      },
      "outputs": [],
      "source": [
        "skaffold_template = f\"\"\"\n",
        "apiVersion: skaffold/v2beta3\n",
        "kind: Config\n",
        "metadata:\n",
        "  name: my-pipeline\n",
        "build:\n",
        "  artifacts:\n",
        "  - image: gcr.io/{PROJECT_ID}/caip-tfx-custom-task-based\n",
        "    context: .\n",
        "    docker:\n",
        "      dockerfile: Dockerfile.task_based\n",
        "  tagPolicy:\n",
        "    envTemplate:\n",
        "      template: \"{{SK_TEMPLATE}}\"\n",
        "\"\"\"\n",
        "with open('skaffold_task_based.yaml', 'w') as f:\n",
        "    f.write(skaffold_template.format(**globals()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2LrjsjExVUxa"
      },
      "source": [
        "Build a container image for the new component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Of1ly6YSVUxa"
      },
      "outputs": [],
      "source": [
        "!skaffold build -f skaffold_task_based.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WHboVgVzVUxd"
      },
      "source": [
        "Let's create a pipeline with this simple component. Note the `add_upstream_node` call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IMIX6_aGVUxd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from tfx.orchestration.ai_platform_pipelines import ai_platform_pipelines_dag_runner\n",
        "\n",
        "from task_based_step import MyTaskBasedStep\n",
        "\n",
        "def task_based_pipeline():\n",
        "    step_1 = MyTaskBasedStep(step_number=1, contents=\"This is step 1\")\n",
        "    step_2 = MyTaskBasedStep(\n",
        "        step_number=2, \n",
        "        contents=\"This is step 2\", \n",
        "        instance_name='MyTaskBasedStep2')\n",
        "    step_2.add_upstream_node(step_1)\n",
        "     \n",
        "    pipeline_name = \"task_dependency_based_pipeline_{}\".format(USER)\n",
        "    return Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=os.path.join(PIPELINE_ROOT, pipeline_name),\n",
        "        components=[step_1, step_2])\n",
        "\n",
        "task_based_pipeline = task_based_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SxG4x2bfVUxh"
      },
      "source": [
        "Now we'll run our pipeline.  You'll see that `step_2` waits until `step_1` has completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DCeOo5vKVUxi"
      },
      "outputs": [],
      "source": [
        "config = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunnerConfig(\n",
        "    project_id=PROJECT_ID,\n",
        "    display_name='task-based-pipeline-{}'.format(USER),\n",
        "    default_image='gcr.io/{}/caip-tfx-custom-task-based:{}'.format(PROJECT_ID, USER))\n",
        "\n",
        "runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(config=config)\n",
        "runner.run(task_based_pipeline, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rTjsz8a3VUxp"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "If you like, you can do some cleanup to avoid storage costs.\n",
        "\n",
        "To remove the files from your GCS bucket, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s4jODl4qVUxp"
      },
      "outputs": [],
      "source": [
        "!gsutil rm 'gs://{BUCKET_NAME}/**'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5sV2FXWnVUxv"
      },
      "source": [
        "You can remove your GCR container images by visiting the [Container Registry](https://console.cloud.google.com/gcr/) panel in the Cloud Console.  Click on an image name to list and remove any of its versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n9R7p4zqVUxx"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook showed examples of how to use custom functions and containers on AI Platform Managed Pipelines, and how to specify task-based dependencies between pipeline steps.\n",
        "\n",
        "You can also explore notebooks that show how to specify TFX pipelines using prebuilt components; and how to run a TFX Templates pipeline on Managed Pipelines. See the EAP guide for the links.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "caipp_eap_custom.ipynb",
      "provenance": [
        {
          "file_id": "17Ye1xgZiHyVx1Fd_4JqDyq2fprYuWhvg",
          "timestamp": 1592238906065
        }
      ],
      "toc_visible": true
    },
    "environment": {
      "name": "tf22-gpu.2-2.m47",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf22-gpu.2-2:m47"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
