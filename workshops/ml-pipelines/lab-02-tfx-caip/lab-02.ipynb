{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training with TFX and Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab demonstrates how to develop a Managed Pipelines pipeline that uses **AI Platform** and **Cloud Dataflow** as executors to run the TFX components at scale. You will also learn how to structure your pipeline code and how to use **TFX CLI** to compile and deploy the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "### Verify TFX SDK Version\n",
    "\n",
    "*Note**: this lab was developed and tested with the following TF ecosystem package versions:\n",
    "\n",
    "`Tensorflow Version: 2.3.0`  \n",
    "`TFX Version: 0.23.0.caip20200818`  \n",
    "`TFDV Version: 0.23.0`  \n",
    "`TFMA Version: 0.23.0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.3.0\n",
      "TFX Version: 0.23.0.caip20200818\n",
      "TFDV Version: 0.23.0\n",
      "TFMA Version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tfx\n",
    "\n",
    "from tfx.tools.cli.ai_platform_pipelines import labels\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"TFX Version:\", tfx.__version__)\n",
    "print(\"TFDV Version:\", tfdv.__version__)\n",
    "print(\"TFMA Version:\", tfma.VERSION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAIPP_API_KEY_ENV',\n",
       " 'CAIPP_ENGINE',\n",
       " 'CAIPP_GCP_PROJECT_ID_ENV',\n",
       " 'CAIPP_RUN_FLAG_ENV',\n",
       " 'CAIPP_TFX_IMAGE_ENV',\n",
       " 'JOB_NAME',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the versions above do not match, update your packages in the current Jupyter kernel. \n",
    "\n",
    "### Update `PATH` with the location of TFX SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] += os.pathsep + '/home/jupyter/.local/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design\n",
    "The pipeline source code can be found in the `pipeline` and `modules` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28\n",
      "drwxr-xr-x 4 jupyter jupyter 4096 Aug 31 00:25 .\n",
      "drwxr-xr-x 6 jupyter jupyter 4096 Aug 31 01:36 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 1106 Aug 31 01:32 configs.py\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Aug 30 01:57 __init__.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Aug 29 22:00 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 jupyter jupyter 7981 Aug 30 02:32 pipeline.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Aug 31 01:33 __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls -la pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline` folder contains the pipeline DSL and configurations.\n",
    "\n",
    "The `config.py` module configures the default values for the pipeline's settings.\n",
    "The default values can be overwritten at compile time by using environment variables.\n",
    "\n",
    "The `pipeline.py` module contains the TFX DSL defining the workflow implemented by the pipeline.\n",
    "\n",
    "The `beam_runner.py` configures the Beam runner used for local testing.\n",
    "\n",
    "The `ml_runner.py` configures the Managed Pipelines runner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Aug 30 01:57 .\n",
      "drwxr-xr-x 6 jupyter jupyter 4096 Aug 31 01:36 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 1222 Aug 29 21:59 features.py\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Aug 30 01:57 __init__.py\n",
      "-rw-r--r-- 1 jupyter jupyter 7302 Aug 29 21:59 model.py\n",
      "-rw-r--r-- 1 jupyter jupyter 2032 Aug 29 21:59 preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "!ls -la modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `modules` folder contains user code for `Transform` and `Trainer` components.\n",
    "\n",
    "\n",
    "The `preprocessing.py` module implements the data preprocessing logic  the `Transform` component.\n",
    "\n",
    "The `model.py` module implements the training logic for the   `Train` component.\n",
    "\n",
    "The `features.py` module contains common definitions for the `model.py` and `preprocessing.py` modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline\n",
    "\n",
    "You will use TFX CLI to compile and deploy the pipeline. As noted in the previous section, the environment specific settings can be updated by modifying the `config.py` file or setting respective environment variables.\n",
    "\n",
    "### Set the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "API_KEY = 'AIzaSyC3Mxax2j15dD8vWxAhe6riGAqAasOEi-U'\n",
    "\n",
    "PIPELINE_NAME = 'tfx_covertype_continuous_training'\n",
    "ARTIFACT_STORE = 'gs://mlops-dev-env-artifact-store'\n",
    "DATA_ROOT = 'gs://workshop-datasets/covertype/small'\n",
    "\n",
    "TARGET_IMAGE = f'gcr.io/{PROJECT_ID}/caip-tfx-custom'\n",
    "BASE_IMAGE = 'gcr.io/caip-pipelines-assets/tfx:latest'\n",
    "\n",
    "#MODEL_NAME = 'tfx_covertype_classifier'\n",
    "\n",
    "#CUSTOM_TFX_IMAGE = 'gcr.io/{}/{}'.format(PROJECT_ID, PIPELINE_NAME)\n",
    "#RUNTIME_VERSION = '2.1'\n",
    "#PYTHON_VERSION = '3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_NAME=tfx_covertype_continuous_training\n",
      "env: ARTIFACT_STORE=gs://mlops-dev-env-artifact-store\n",
      "env: DATA_ROOT=gs://workshop-datasets/covertype/small\n"
     ]
    }
   ],
   "source": [
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env ARTIFACT_STORE={ARTIFACT_STORE}\n",
    "%env DATA_ROOT={DATA_ROOT}\n",
    "\n",
    "\n",
    "#%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "\n",
    "#%env GCP_REGION={GCP_REGION}\n",
    "#%env MODEL_NAME={MODEL_NAME}\n",
    "\n",
    "#%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "#%env PYTHON_VERIONS={PYTHON_VERSION}\n",
    "#%env USE_KFP_SA={USE_KFP_SA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Currently there is an issue with TFX CLI and environment variables. As a temporary mitigation update the `pipeline/configs.py` with equivalent values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/configs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/configs.py\n",
    "# Copyright 2020 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"The pipeline configurations.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "PIPELINE_NAME=os.getenv(\"PIPELINE_NAME\", \"tfx_covertype_continuous_training\")\n",
    "ARTIFACT_STORE=os.getenv(\"ARTIFACT_STORE\", \"gs://mlops-dev-env-artifact-store\")\n",
    "DATA_ROOT=os.getenv(\"DATA_ROOT\", \"gs://workshop-datasets/covertype/small\")\n",
    "#MODEL_NAME=os.getenv(\"MODEL_NAME\", \"covertype_classifier\")\n",
    "#GCP_REGION=os.getenv(\"GCP_REGION\", \"us-central1\")\n",
    "#RUNTIME_VERSION=os.getenv(\"RUNTIME_VERSION\", \"2.1\")\n",
    "#PYTHON_VERSION=os.getenv(\"PYTHON_VERSION\", \"3.7\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "CLI\n",
      "Cloud AI Platform Pipelines\n",
      "Creating pipeline\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/mlops-dev-env/caip-tfx-custom is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/mlops-dev-env/caip-tfx-custom -> gcr.io/mlops-dev-env/caip-tfx-custom:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/mlops-dev-env/caip-tfx-custom: Not found. Building\n",
      "[Skaffold] Building [gcr.io/mlops-dev-env/caip-tfx-custom]...\n",
      "[Skaffold] Sending build context to Docker daemon  102.4kB\n",
      "[Skaffold] Step 1/4 : FROM gcr.io/caip-pipelines-assets/tfx:latest\n",
      "[Skaffold]  ---> 19c14dda2bb5\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 73280faa1be8\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 4a6c8f2b9235\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in eee0cc84a5b7\n",
      "[Skaffold]  ---> 7a6906fafa9f\n",
      "[Skaffold] Successfully built 7a6906fafa9f\n",
      "[Skaffold] Successfully tagged gcr.io/mlops-dev-env/caip-tfx-custom:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/mlops-dev-env/caip-tfx-custom]\n",
      "[Skaffold] 79670b28ea60: Preparing\n",
      "[Skaffold] 096a31859079: Preparing\n",
      "[Skaffold] 040e39b76df7: Preparing\n",
      "[Skaffold] 34d223592bf6: Preparing\n",
      "[Skaffold] 076274ac0c03: Preparing\n",
      "[Skaffold] d73a82f851f0: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] d73a82f851f0: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 040e39b76df7: Layer already exists\n",
      "[Skaffold] 34d223592bf6: Layer already exists\n",
      "[Skaffold] 076274ac0c03: Layer already exists\n",
      "[Skaffold] 096a31859079: Layer already exists\n",
      "[Skaffold] d73a82f851f0: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 79670b28ea60: Pushed\n",
      "[Skaffold] latest: digest: sha256:118201355dad51d23d1a615b5389a53a57aef7476a408e04e1166002df4f6a99 size: 3265\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "****************\n",
      "gcr.io/mlops-dev-env/caip-tfx-custom\n",
      "None\n",
      "\n",
      "gs://mlops-dev-env-artifact-store\n",
      "gs://mlops-dev-env-artifact-store\n",
      "*****************\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Runner compile\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline \"tfx_covertype_continuous_training\" created successfully.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx caipp pipeline create  \\\n",
    "--pipeline_path=runner.py \\\n",
    "--build-base-image={BASE_IMAGE} \\\n",
    "--build-target-image={TARGET_IMAGE} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to redeploy the pipeline you can first delete the previous version using `tfx pipeline delete` or you can update the pipeline in-place using `tfx pipeline update`.\n",
    "\n",
    "To delete the pipeline:\n",
    "\n",
    "`tfx caipp pipeline delete --pipeline_name {PIPELINE_NAME}`\n",
    "\n",
    "To update the pipeline:\n",
    "\n",
    "`tfx caipp pipeline update --pipeline_path runner.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CONDA_SHLVL': '1',\n",
       " 'LD_LIBRARY_PATH': '/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64',\n",
       " 'CONDA_EXE': '/opt/conda/bin/conda',\n",
       " 'RESTRICTION_TYPE_FILE_PATH': '/opt/deeplearning/restriction',\n",
       " 'LANG': 'en_US.UTF-8',\n",
       " 'DL_PATH': '/opt/deeplearning',\n",
       " 'OS_UBUNTU1804': 'ubuntu-1804-lts',\n",
       " 'INVOCATION_ID': '47f3f3ed8b66414e94558cde8b8ceda8',\n",
       " 'GSETTINGS_SCHEMA_DIR_CONDA_BACKUP': '',\n",
       " 'CONDA_PREFIX': '/opt/conda',\n",
       " 'OS_DEBIAN9': 'debian-9',\n",
       " 'JUPYTER_DEPS_PATH': '/opt/deeplearning/jupyter',\n",
       " 'WORKSPACE_PATH': '/opt/deeplearning/workspace',\n",
       " 'TUTORIALS_PATH': '/opt/deeplearning/workspace/tutorials',\n",
       " '_CE_M': '',\n",
       " 'USER': 'jupyter',\n",
       " 'FRAMEWORK_FILE_PATH': '/opt/deeplearning/metadata/framework',\n",
       " 'ENV_URI_FILE_PATH': '/opt/deeplearning/metadata/env_uri',\n",
       " 'PWD': '/home/jupyter',\n",
       " 'VERSION_FILE_PATH': '/opt/deeplearning/metadata/version',\n",
       " 'HOME': '/home/jupyter',\n",
       " 'CONDA_PYTHON_EXE': '/opt/conda/bin/python',\n",
       " 'JOURNAL_STREAM': '8:18043',\n",
       " 'DL_ANACONDA_HOME': '/opt/conda',\n",
       " 'OPAL_PREFIX': '/usr',\n",
       " '_CE_CONDA': '',\n",
       " 'GSETTINGS_SCHEMA_DIR': '/opt/conda/share/glib-2.0/schemas',\n",
       " 'ENV_VERSION_FILE_PATH': '/opt/deeplearning/metadata/env_version',\n",
       " 'DL_BIN_PATH': '/opt/deeplearning/bin',\n",
       " 'CONDA_PROMPT_MODIFIER': '(base) ',\n",
       " 'OS_IMAGE_FAMILY': 'debian-9',\n",
       " 'OS_DEBIAN10': 'debian-10',\n",
       " 'TF_BINARIES_FOLDER': '/opt/deeplearning/binaries/tensorflow',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'OS_IMAGE_PROJECT': 'debian-cloud',\n",
       " 'TITLE_FILE_PATH': '/opt/deeplearning/metadata/title',\n",
       " 'OS_DEBIAN_PROJECT': 'debian-cloud',\n",
       " 'SRC_PATH': '/opt/deeplearning/src',\n",
       " 'SHLVL': '1',\n",
       " 'DL_METADATA_PATH': '/opt/deeplearning/metadata',\n",
       " 'BINARIES_PATH': '/opt/deeplearning/binaries',\n",
       " 'LOGNAME': 'jupyter',\n",
       " 'PATH': '/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin',\n",
       " 'DL_PATH_DEPS': '/opt/deeplearning/deps',\n",
       " 'CONDA_DEFAULT_ENV': 'base',\n",
       " 'TF_BINARIES_VERSION': '2-3',\n",
       " '_': '/opt/conda/bin/jupyter',\n",
       " 'GIT_PYTHON_REFRESH': 'quiet',\n",
       " 'JPY_PARENT_PID': '1910',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'NO_GCE_CHECK': 'False',\n",
       " 'GCE_METADATA_TIMEOUT': '3',\n",
       " 'TF2_BEHAVIOR': '1',\n",
       " 'KMP_INIT_AT_FORK': 'FALSE',\n",
       " 'PIPELINE_NAME': 'tfx_covertype_continuous_training',\n",
       " 'ARTIFACT_STORE': 'gs://mlops-dev-env-artifact-store',\n",
       " 'DATA_ROOT': 'gs://workshop-datasets/covertype/small'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "CLI\n",
      "Cloud AI Platform Pipelines\n",
      "Creating a run for pipeline: tfx_covertype_continuous_training\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n",
      "****************\n",
      "gcr.io/mlops-dev-env/caip-tfx-custom\n",
      "AIzaSyC3Mxax2j15dD8vWxAhe6riGAqAasOEi-U\n",
      "mlops-dev-env\n",
      "gs://mlops-workshop-artifact-store\n",
      "None\n",
      "*****************\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "Runner run\n",
      "INFO:absl:Compiled JSON request: {\"name\": \"projects/mlops-dev-env/pipelineJobs/covertype_continuous_training_20200831014013\", \"displayName\": \"covertype_continuous_training\", \"spec\": {\"pipelineContext\": \"covertype_continuous_training\", \"steps\": {\"StatisticsGen\": {\"task\": {\"inputs\": {\"examples\": {\"stepOutput\": {\"step\": \"CsvExampleGen\", \"output\": \"examples\"}}}, \"executionProperties\": {\"exclude_splits\": {\"stringValue\": \"[]\"}}, \"outputs\": {\"statistics\": {\"artifact\": {\"customProperties\": {\"custom:name\": {\"stringValue\": \"statistics\"}, \"custom:pipeline_name\": {\"stringValue\": \"covertype_continuous_training\"}, \"tfx_type\": {\"stringValue\": \"tfx.types.standard_artifacts.ExampleStatistics\"}, \"type_name\": {\"stringValue\": \"ExampleStatistics\"}, \"custom:producer_component\": {\"stringValue\": \"StatisticsGen\"}}, \"statistics\": {}}}}, \"container\": {\"image\": \"gcr.io/mlops-dev-env/caip-tfx-custom\", \"command\": [\"python3\", \"/tfx-src/tfx/orchestration/ai_platform_pipelines/container/ai_platform_run_executor.py\"], \"args\": [\"--executor_class_path\", \"tfx.components.statistics_gen.executor.Executor\", \"--json_serialized_invocation_args\", \"{{$}}\"]}}, \"dependencies\": [{\"step\": \"CsvExampleGen\"}], \"cachePolicy\": {}}, \"CsvExampleGen\": {\"fileBasedExampleGen\": {\"inputBaseUri\": \"gs://workshop-datasets/covertype/small\", \"inputConfig\": {\"splits\": [{\"pattern\": \"*\", \"name\": \"single_split\"}]}, \"outputConfig\": {\"split_config\": {\"splits\": [{\"name\": \"train\", \"hash_buckets\": 4.0}, {\"hash_buckets\": 1.0, \"name\": \"eval\"}]}}, \"container\": {\"image\": \"gcr.io/mlops-dev-env/caip-tfx-custom\", \"command\": [\"python3\", \"/tfx-src/tfx/orchestration/ai_platform_pipelines/container/ai_platform_run_executor.py\"], \"args\": [\"--executor_class_path\", \"tfx.components.example_gen.csv_example_gen.executor.Executor\", \"--json_serialized_invocation_args\", \"{{$}}\"]}}, \"cachePolicy\": {}}}}, \"outputPathConfig\": {\"pipelineRoot\": \"gs://mlops-workshop-artifact-store/covertype_continuous_training\"}}\n",
      "WARNING:googleapiclient.http:Invalid JSON content from response: b'{\\n  \"error\": {\\n    \"code\": 403,\\n    \"message\": \"Field: \\'OutputPathConfig.pipeline_root\\'; Value: \\'gs://mlops-workshop-artifact-store/covertype_continuous_training\\'; Reason: ValidationError; Detail: service-881178567352@cloud-ml-alpha-robot.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\",\\n    \"status\": \"PERMISSION_DENIED\"\\n  }\\n}\\n'\n",
      "Traceback (most recent call last):\n",
      "  File \"runner.py\", line 69, in <module>\n",
      "    api_key=api_key)\n",
      "  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/orchestration/ai_platform_pipelines/ai_platform_pipelines_dag_runner.py\", line 299, in run\n",
      "    response = request.execute()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/googleapiclient/_helpers.py\", line 134, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/googleapiclient/http.py\", line 907, in execute\n",
      "    raise HttpError(resp, content, uri=self.uri)\n",
      "googleapiclient.errors.HttpError: <HttpError 403 when requesting https://alpha-ml.googleapis.com/v1/projects/mlops-dev-env/pipelineJobs?alt=json returned \"Field: 'OutputPathConfig.pipeline_root'; Value: 'gs://mlops-workshop-artifact-store/covertype_continuous_training'; Reason: ValidationError; Detail: service-881178567352@cloud-ml-alpha-robot.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.\">\n",
      "\u001b[0mError while running \"/opt/conda/bin/python3.7 runner.py\" \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx caipp run create \\\n",
    "--pipeline-name={PIPELINE_NAME} \\\n",
    "--project-id={PROJECT_ID} \\\n",
    "--api-key={API_KEY} \\\n",
    "--target-image={TARGET_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all active runs of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx caipp run list --pipeline_name {PIPELINE_NAME} --api-key {API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the status of a given run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID='[YOUR RUN ID]'\n",
    "\n",
    "!tfx caipp run status --pipeline_name {PIPELINE_NAME} \\\n",
    "--run_id {RUN_ID} \\\n",
    "--api-key {API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you learned how to manually build and deploy a TFX pipeline to AI Platform Pipelines and trigger pipeline runs from a notebook. In the next lab, you will construct a Cloud Build CI/CD workflow that automatically builds and deploys this same TFX pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
