{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4XVCP4WZAwtS"
      },
      "source": [
        "##### Copyright \u0026copy; 2020 Google Inc.\n",
        "\n",
        "\u003cfont size=-1\u003eLicensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.\u003c/font\u003e\n",
        "\u003chr/\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wHJGbAteAwtT"
      },
      "source": [
        "# Managed Pipelines EAP: Create and run a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLQ6UIWPAwtU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[AI Platform Pipelines - Managed (Managed Pipelines)](https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#) makes it easier for you to run your ML Pipelines in a scalable and cost-effective way, while offering you ‘no lock-in’ flexibility. You build your pipelines in Python using [TensorFlow Extended (TFX)](tensorflow.org/tfx), and then execute your pipelines on Google Cloud serverlessly. You don’t have to worry about scale and only pay for what you use. (You can also take the same TFX pipelines and run them using Kubeflow Pipelines).\n",
        "\n",
        "This notebook shows an example of how to use AI Platform Pipelines.   \n",
        "The notebook is designed to run on AI Platform Notebooks. If you want to run this notebook in your own development environment, you will need to do a bit more setup first.  See [these instructions](\u003chttps://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#heading=h.pyk4nfqsszzz\u003e).  \n",
        "\n",
        "### About the dataset and ML Task\n",
        "\n",
        "You will build a pipeline using a [Chicago Taxi Trips public dataset](\n",
        "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew).  The task is to learn a model that predicts whether the tip was \u003e= 20% of the fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wx4ToiruAwtU"
      },
      "source": [
        "## Step 1: Follow the 'before you begin' steps in the Managed Pipelines User Guide\n",
        "\n",
        "Before proceeeding, make sure that you've followed all the steps in the [\"Before you Begin\" section](https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#heading=h.65kbhyyf93x0) of the Managed Pipelines User Guide.  You'll need to use the API key that you created for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOK0VEyCAwtV"
      },
      "source": [
        "## Step 2: set up your environment\n",
        "\n",
        "First, ensure that Python 3 is being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E-2B6dGcAwtW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hMijcmnOAwta"
      },
      "source": [
        "### Install the TFX SDK\n",
        "\n",
        "Next, we'll upgrade pip and install the TFX SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JxsDEJ1ZAwta"
      },
      "outputs": [],
      "source": [
        "SDK_LOCATION = 'gs://caip-pipelines-sdk/releases/20200727/tfx-0.22.0.caip20200727-py3-none-any.whl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CgWh9LlhAwtd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pip --upgrade\n",
        "!gsutil cp {SDK_LOCATION} /tmp/tfx-0.22.0.caip20200727-py3-none-any.whl \u0026\u0026 pip install --no-cache-dir /tmp/tfx-0.22.0.caip20200727-py3-none-any.whl\n",
        "\n",
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ys2NiNb1Awtg"
      },
      "source": [
        "Ensure that you can import TFX and that its version is \u003e= 0.22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uOPbDPGdAwth"
      },
      "outputs": [],
      "source": [
        "# Check version\n",
        "import tfx\n",
        "tfx.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LzIxhmntAwtj"
      },
      "source": [
        "### Identify or Create a GCS bucket to use for your pipeline\n",
        "\n",
        "Below, you will need to specify a Google Gloud Storage (GCS) bucket for the Pipelines run to use.  If you do not already have one that you want to use, you can [create a new bucket](https://cloud.google.com/storage/docs/creating-buckets)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1o8r7xcBAwtk"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Let's set up some variables used to customize the pipelines below. **Before you execute the following cell, make the indicated 'Change this' edits**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6mSrdChKAwtl"
      },
      "outputs": [],
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "    \n",
        "USER = 'YOUR_USERNAME'  # Change this to your username.\n",
        "BUCKET_NAME = 'YOUR_GCS_BUCKET'  # Change this to your GCS bucket name.  Do not include the `gs://`\n",
        "\n",
        "# It is not necessary to append your username to the pipeline root, \n",
        "# but this may be useful if multiple people are using the same project.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID' # Change this to your project id\n",
        "BASE_IMAGE = 'gcr.io/caip-pipelines-assets/tfx:0.22.0.caip20200727'\n",
        "\n",
        "API_KEY = 'YOUR_API_KEY'  # Change this to the API key that you created during initial setup\n",
        "# ENDPOINT = 'alpha-ml.googleapis.com'  # this is the default during EAP\n",
        "\n",
        "PIPELINE_ROOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1yU7nECTAwtn"
      },
      "source": [
        "## Step 3: Run the 'Chicago Taxi' Pipeline\n",
        "\n",
        "In this section, we'll run the canonical Chicago Taxi Pipeline.\n",
        "\n",
        "We'll first do some imports. You can ignore the `RuntimeParameter` warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Xot81Np8Awto"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional, Text\n",
        "\n",
        "import os\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import InfraValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.dsl.experimental import latest_artifacts_resolver\n",
        "from tfx.orchestration import pipeline as tfx_pipeline\n",
        "from tfx.orchestration.ai_platform_pipelines import ai_platform_pipelines_dag_runner\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import standard_artifacts\n",
        "from tfx.utils import dsl_utils\n",
        "from tfx.types import channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cboAXInAwtr"
      },
      "source": [
        "Next, we'll set some variables to define our data sources.\n",
        "\n",
        "We're defining both a [BigQuery](https://cloud.google.com/bigquery/docs/) query and the path to a folder of CSV data.  Below, we'll show examples of how to use each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ex79wk7YAwts"
      },
      "outputs": [],
      "source": [
        "# Define the query used for BigQueryExampleGen.\n",
        "QUERY = \"\"\"\n",
        "        SELECT\n",
        "          pickup_community_area,\n",
        "          fare,\n",
        "          EXTRACT(MONTH FROM trip_start_timestamp) AS trip_start_month,\n",
        "          EXTRACT(HOUR FROM trip_start_timestamp) AS trip_start_hour,\n",
        "          EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS trip_start_day,\n",
        "          UNIX_SECONDS(trip_start_timestamp) AS trip_start_timestamp,\n",
        "          pickup_latitude,\n",
        "          pickup_longitude,\n",
        "          dropoff_latitude,\n",
        "          dropoff_longitude,\n",
        "          trip_miles,\n",
        "          pickup_census_tract,\n",
        "          dropoff_census_tract,\n",
        "          payment_type,\n",
        "          company,\n",
        "          trip_seconds,\n",
        "          dropoff_community_area,\n",
        "          tips\n",
        "        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
        "        WHERE (ABS(FARM_FINGERPRINT(unique_key)) / 0x7FFFFFFFFFFFFFFF)\n",
        "          \u003c 0.000001\"\"\"\n",
        "\n",
        "# Data location for the CsvExampleGen. The content of the data is equivalent\n",
        "# to the query above.\n",
        "CSV_INPUT_PATH = 'gs://ml-pipeline/sample-data/chicago-taxi/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "afgqS0ouAwtv"
      },
      "source": [
        "Now we're ready to build and run a TFX pipeline. If you look at the helper function below, you'll notice that it's using the `BigQueryExampleGen` component if `query` is defined.  This means that we'll get our data from BigQuery for this first pipeline run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PZ80cn_mAwtw"
      },
      "outputs": [],
      "source": [
        "# Create a helper function to construct a TFX pipeline.\n",
        "def create_tfx_pipeline(\n",
        "    query: Optional[Text] = None,\n",
        "    input_path: Optional[Text] = None,\n",
        "):\n",
        "  \"\"\"Creates an end-to-end Chicago Taxi pipeline in TFX.\"\"\"\n",
        "  if bool(query) == bool(input_path):\n",
        "    raise ValueError('Exact one of query or input_path is expected.')\n",
        "\n",
        "  if query:\n",
        "    example_gen = BigQueryExampleGen(query=query)\n",
        "  else:\n",
        "    example_gen = CsvExampleGen(input=dsl_utils.external_input(input_path))\n",
        "\n",
        "  beam_pipeline_args = [\n",
        "      # Uncomment to use Dataflow.\n",
        "      # '--runner=DataflowRunner',\n",
        "      # '--experiments=shuffle_mode=auto',\n",
        "      # '--region=us-central1',\n",
        "      # '--disk_size_gb=100',\n",
        "      '--temp_location=' + os.path.join(PIPELINE_ROOT, 'dataflow', 'temp'),\n",
        "      '--project={}'.format(PROJECT_ID)  # Always needed for BigQueryExampleGen.\n",
        "  ]\n",
        "\n",
        "  # Use a module file built-in the TFX image to make sure things are in sync.\n",
        "  module_file = '/tfx-src/tfx/examples/chicago_taxi_pipeline/taxi_utils.py'\n",
        "\n",
        "  statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
        "  schema_gen = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      infer_feature_shape=False)\n",
        "  example_validator = ExampleValidator(\n",
        "      statistics=statistics_gen.outputs['statistics'],\n",
        "      schema=schema_gen.outputs['schema'])\n",
        "  transform = Transform(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      module_file=module_file)\n",
        "\n",
        "  trainer = Trainer(\n",
        "      transformed_examples=transform.outputs['transformed_examples'],\n",
        "      schema=schema_gen.outputs['schema'],\n",
        "      transform_graph=transform.outputs['transform_graph'],\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=10),\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=5),\n",
        "      module_file=module_file,\n",
        "  )\n",
        "\n",
        "  # Set the TFMA config for Model Evaluation and Validation.\n",
        "  eval_config = tfma.EvalConfig(\n",
        "      model_specs=[tfma.ModelSpec(signature_name='eval')],\n",
        "      metrics_specs=[\n",
        "          tfma.MetricsSpec(\n",
        "              metrics=[tfma.MetricConfig(class_name='ExampleCount')],\n",
        "              thresholds={\n",
        "                  'binary_accuracy':\n",
        "                      tfma.MetricThreshold(\n",
        "                          value_threshold=tfma.GenericValueThreshold(\n",
        "                              lower_bound={'value': 0.5}),\n",
        "                          change_threshold=tfma.GenericChangeThreshold(\n",
        "                              direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                              absolute={'value': -1e-10}))\n",
        "              })\n",
        "      ],\n",
        "      slicing_specs=[\n",
        "          tfma.SlicingSpec(),\n",
        "          tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
        "      ])\n",
        "\n",
        "  evaluator = Evaluator(\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      model=trainer.outputs['model'],\n",
        "      eval_config=eval_config)\n",
        "\n",
        "  pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      model_blessing=evaluator.outputs['blessing'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=os.path.join(PIPELINE_ROOT, 'model_serving'))))\n",
        "\n",
        "  components=[\n",
        "      example_gen, statistics_gen, schema_gen, example_validator, transform,\n",
        "      trainer, evaluator, pusher\n",
        "  ]\n",
        "\n",
        "  return tfx_pipeline.Pipeline(\n",
        "      pipeline_name='taxi-pipeline-{}'.format(USER),\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      enable_cache=True,\n",
        "      components=components,\n",
        "      beam_pipeline_args=beam_pipeline_args\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZ6pshV_Awtz"
      },
      "source": [
        "We'll call the helper function to create the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "R6ToEm1FAwtz"
      },
      "outputs": [],
      "source": [
        "bigquery_taxi_pipeline = create_tfx_pipeline(query=QUERY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h6vgtSKRAwt4"
      },
      "source": [
        "### Step 3.1: Run the pipeline using BigQuery-based example generation\n",
        "\n",
        "Now we're ready to run the pipeline! As you can see below, we're configuring the runner to use AI Platform Pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Yyj0_VfcAwt5"
      },
      "outputs": [],
      "source": [
        "runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(\n",
        "    config=ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunnerConfig(\n",
        "        project_id=PROJECT_ID,\n",
        "        display_name='big-query-taxi-pipeline-{}'.format(USER),\n",
        "        default_image=BASE_IMAGE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "f4MSLBw_Awt7"
      },
      "outputs": [],
      "source": [
        "runner.run(bigquery_taxi_pipeline, api_key=API_KEY)\n",
        "# If you want to inspect the pipeline proto, run the following and look at the file contents.\n",
        "# runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(config=config, output_filename='pipeline.json')\n",
        "# runner.compile(taxi_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MxGLbqOkAwt9"
      },
      "source": [
        "See the Pipeline job [here](https://console.cloud.google.com/ai-platform/pipelines/runs).\n",
        "\n",
        "See the CMLE steps [here](https://console.cloud.google.com/ai-platform/jobs).  This is where you can monitor the details of the pipeline component executions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G8WVtSTwAwt-"
      },
      "source": [
        "### Step 3.2: Run the pipeline using file-based example generation\n",
        "\n",
        "Start another run that uses file-based example generation instead of BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XHs4YPqxAwt-"
      },
      "outputs": [],
      "source": [
        "file_based_example_gen_taxi_pipeline = create_tfx_pipeline(input_path=CSV_INPUT_PATH)\n",
        "\n",
        "runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(\n",
        "    config=ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunnerConfig(\n",
        "        project_id=PROJECT_ID,\n",
        "        display_name='fbeg-taxi-pipeline-{}'.format(USER),\n",
        "        default_image=BASE_IMAGE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vrFNAM8qAwuB"
      },
      "outputs": [],
      "source": [
        "runner.run(file_based_example_gen_taxi_pipeline, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zxg3jeOpAwuE"
      },
      "source": [
        "## Step 4: Explore Caching\n",
        "\n",
        "In Step 3, the pipelines were run with caching enabled. You can see that the helper function above sets `enable_cache=True` when creating the Pipeline object.  \n",
        "\n",
        "Let's run the Step 3.1 pipeline again. **Wait until the first job  is done** (as confirmed in the Cloud Console UI) before running the next cell. You should see the run below complete more quickly in the Console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Sfqz93mLAwuE"
      },
      "outputs": [],
      "source": [
        "# run this after the job from step 3.1 has finished\n",
        "runner = ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunner(\n",
        "    config=ai_platform_pipelines_dag_runner.AIPlatformPipelinesDagRunnerConfig(\n",
        "        project_id=PROJECT_ID,\n",
        "        display_name='big-query-taxi-pipeline-{}'.format(USER),\n",
        "        default_image=BASE_IMAGE))\n",
        "\n",
        "runner.run(bigquery_taxi_pipeline, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "exZBVvMwAwuH"
      },
      "source": [
        "If you like, disable the cache and run it again. This time, it should re-run all steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7HnHXDyfAwuH"
      },
      "outputs": [],
      "source": [
        "bigquery_taxi_pipeline.enable_cache = False\n",
        "runner.run(bigquery_taxi_pipeline, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lq6M5J5xAwuK"
      },
      "source": [
        "\u003e Note: The `CsvExampleGen` component used for the pipeline in Step 3.2 does not support caching as of this writing, but will soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZuRNyFChAwuK"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "If you like, you can do some cleanup to avoid storage costs.\n",
        "\n",
        "To remove the files from your GCS bucket, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uphv47LNAwuL"
      },
      "outputs": [],
      "source": [
        "!gsutil rm 'gs://{BUCKET_NAME}/**'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KX2ZmkKWAwuN"
      },
      "source": [
        "You can remove your GCR container images by visiting the [Container Registry](https://console.cloud.google.com/gcr/) panel in the Cloud Console.  Click on an image name to list and remove any of its versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tLOEdO63AwuN"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook showed examples of defining TFX pipelines using prebuilt components, and running them on AI Platform Managed Pipelines.\n",
        "\n",
        "Next, explore notebooks that show how to use custom functions and containers; and how to run a TFX Templates pipeline on Managed Pipelines. See the EAP guide for the links."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "caipp_eap_intro.ipynb",
      "provenance": [
        {
          "file_id": "17RQspMthTgA6DR-Cqv7u8FWDMaDHBTHB",
          "timestamp": 1592238866852
        }
      ]
    },
    "environment": {
      "name": "tf22-gpu.2-2.m47",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf22-gpu.2-2:m47"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
