{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ydsPE-aN1Alm"
      },
      "source": [
        "##### Copyright &copy; 2020 Google Inc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCSByGH6C7zS"
      },
      "source": [
        "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6TyrY7lV0oke"
      },
      "source": [
        "# Managed Pipelines EAP: Create and run a pipeline using TFX Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iLYriYe10okf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "[AI Platform Pipelines - Managed (Managed Pipelines)](https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#) makes it easier for you to run your ML Pipelines in a scalable and cost-effective way, while offering you ‘no lock-in’ flexibility. You build your pipelines in Python using [TensorFlow Extended (TFX)](tensorflow.org/tfx), and then execute your pipelines on Google Cloud serverlessly. You don’t have to worry about scale and only pay for what you use. (You can also take the same TFX pipelines and run them using Kubeflow Pipelines).\n",
        "\n",
        "This notebook shows how to create a TensorFlow Extended (TFX) pipeline, using *templates* provided with the TFX Python package.\n",
        "\n",
        "The notebook is designed to run on AI Platform Notebooks. If you want to run this notebook in your own development environment, you will need to do a bit more setup first.  See [these instructions](<https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#heading=h.pyk4nfqsszzz>).  \n",
        "\n",
        "\n",
        "### About the dataset and ML Task\n",
        "\n",
        "You will build a pipeline using a [Chicago Taxi Trips public dataset](\n",
        "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) released by the city of Chicago.  The task is to learn a model that predicts whether the tip was >= 20% of the fare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mLflvEGvIJG8"
      },
      "source": [
        "## Step 0: Follow the 'before you begin' steps in the Managed Pipelines User Guide\n",
        "\n",
        "Before proceeeding, make sure that you've followed all the steps in the [\"Before you Begin\" section](https://docs.google.com/document/d/1FAyZhXRmZwJ7oCjRZZmzRG-ERYxyZyUQikrjR28Ev4E/edit?ts=5ec30a40#heading=h.65kbhyyf93x0) of the Managed Pipelines User Guide.  You'll need to use the API key that you created for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GUkqKgvKIJG8"
      },
      "source": [
        "## Step 1: set up your environment\n",
        "\n",
        "First, ensure that Python 3 is being used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "72qoluIAIJG9",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:25:08) \\n[GCC 7.5.0]'"
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import sys\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JK_l_XcUIJHB"
      },
      "source": [
        "### Install the TFX SDK\n",
        "\n",
        "Next, we'll upgrade pip and install the TFX SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "W7HK48DeIJHB"
      },
      "outputs": [],
      "source": [
        "SDK_LOCATION = 'gs://caip-pipelines-sdk/releases/20200727/tfx-0.22.0.caip20200727-py3-none-any.whl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qrVxdDsBIJHE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pip --upgrade\n",
        "!gsutil cp {SDK_LOCATION} /tmp/tfx-0.22.0.caip20200727-py3-none-any.whl && pip install --no-cache-dir /tmp/tfx-0.22.0.caip20200727-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTMJu4XXIJHH"
      },
      "source": [
        "Next, install Skaffold.  We'll use it later to help build a container image. \n",
        "\n",
        "> Note: if you're running this notebook in a non-linux local development environment, see [these Skaffold installation instructions](https://skaffold.dev/docs/install/) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UuNBway_IJHH"
      },
      "outputs": [],
      "source": [
        "# Install skaffold.\n",
        "!curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && mkdir -p /home/jupyter/.local/bin && mv skaffold /home/jupyter/.local/bin/\n",
        "\n",
        "# Automatically restart kernel after installs\n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__bfvHcTIJHK"
      },
      "source": [
        "Ensure that you can import TFX and that its version is >= 0.22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "75SR8k9ZIJHL"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'0.23.0.caip20200818'"
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Check version\n",
        "import tfx\n",
        "tfx.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XCFgAvhpIJHO"
      },
      "source": [
        "### Identify or Create a GCS bucket to use for your pipeline\n",
        "\n",
        "Below, you will need to specify a Google Gloud Storage (GCS) bucket for the Pipelines run to use.  If you do not already have one that you want to use, you can [create a new bucket](https://cloud.google.com/storage/docs/creating-buckets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OgxK0s220olZ"
      },
      "outputs": [],
      "source": [
        "# You can see your existing buckets using `gsutil`. The following command will show bucket names without prefix and postfix.\n",
        "!gsutil ls | cut -d / -f 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZR4k0szIJHR"
      },
      "source": [
        "### Set up variables\n",
        "\n",
        "Let's set up some variables used to customize the pipelines below. If you have gcloud installed and configured, as will be the case on AI Platform Notebooks, you can find your GCP Project ID as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "z2dFbOH6IJHS",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "GCP project ID:mlops-dev-env\n"
        }
      ],
      "source": [
        "# Get your GCP project id from gcloud\n",
        "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
        "PROJECT_ID=shell_output[0]\n",
        "print(\"GCP project ID:\" + PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq6Ww0-rIJHU"
      },
      "source": [
        "**Before you execute the following cell, make the indicated 'Change this' edits**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v07GUkQLIJHV",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "env: PATH=/home/jupyter/.vscode-server/bin/a0479759d6e9ea56afa657e454193f72aef85bd0/bin:/usr/local/cuda/bin:/opt/conda/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
        }
      ],
      "source": [
        "PATH=%env PATH\n",
        "%env PATH={PATH}:/home/jupyter/.local/bin\n",
        "    \n",
        "USER = 'JK'  # Change this to your username.\n",
        "BUCKET_NAME = 'mlops-dev-workspace'  # Change this to your GCS bucket name.  Do not include the `gs://`\n",
        "\n",
        "BASE_IMAGE = 'gcr.io/caip-pipelines-assets/tfx:0.23.0.caip20200818'\n",
        "\n",
        "API_KEY = 'AIzaSyC3Mxax2j15dD8vWxAhe6riGAqAasOEi-U'  # Change this to the API key that you created during initial setup\n",
        "# ENDPOINT = 'alpha-ml.googleapis.com'  # this is the default during EAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K6T-KXeA0ok3"
      },
      "source": [
        "Next, we'll set the Docker image name for the image we'll build later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3ztxXOVD0ok4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'gcr.io/mlops-dev-env/tfx-template-JK'"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Set the Docker image name for the pipeline image we'll build\n",
        "CUSTOM_TFX_IMAGE='gcr.io/{}/tfx-template-{}'.format(PROJECT_ID, USER)\n",
        "CUSTOM_TFX_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TOsQbkky0ok7"
      },
      "source": [
        "Now we're ready to create a pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cxlbi1QM0ok8"
      },
      "source": [
        "## Step 2. Copy the predefined template to your project directory.\n",
        "\n",
        "In this step, we will create a working pipeline project directory and files by copying files from a predefined TFX template.\n",
        "\n",
        "You may give your pipeline a different name by changing the `PIPELINE_NAME` below. This will also become the name of the local directory where the TFX Template files will be located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cIPlt-700ok-"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK'"
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "PIPELINE_NAME = 'my_pipeline_{}'.format(USER)\n",
        "import os\n",
        "PROJECT_DIR=os.path.join(os.getcwd(),\"tfx_template\",PIPELINE_NAME)\n",
        "PROJECT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ozHIomcd0olB"
      },
      "source": [
        "TFX includes the `taxi` template with the TFX python package. (If you are planning to solve a point-wise prediction problem, including classification and regresssion, this template could be a useful starting point).\n",
        "\n",
        "The `tfx template copy` CLI command copies predefined template files into your TFX project directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VLXpTTjU0olD",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CLI\nCopying taxi pipeline template\ndata_validation.ipynb -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/data_validation.ipynb\n__init__.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/__init__.py\nai_platform_pipelines_dag_runner.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/ai_platform_pipelines_dag_runner.py\nmodel_analysis.ipynb -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/model_analysis.ipynb\nfeatures.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/features.py\n__init__.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/__init__.py\n__init__.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/estimator/__init__.py\nconstants.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/estimator/constants.py\nmodel_test.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/estimator/model_test.py\nmodel.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/estimator/model.py\n__init__.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/keras/__init__.py\nconstants.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/keras/constants.py\nmodel_test.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/keras/model_test.py\nmodel.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/keras/model.py\nfeatures_test.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/features_test.py\npreprocessing.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/preprocessing.py\npreprocessing_test.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/models/preprocessing_test.py\nkubeflow_dag_runner.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/kubeflow_dag_runner.py\n__init__.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/pipeline/__init__.py\npipeline.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/pipeline/pipeline.py\nconfigs.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/pipeline/configs.py\nbeam_dag_runner.py -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/beam_dag_runner.py\n.gitignore -> /home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK/.gitignore\n\u001b[0m"
        }
      ],
      "source": [
        "!tfx template copy \\\n",
        "  --pipeline-name={PIPELINE_NAME} \\\n",
        "  --destination-path={PROJECT_DIR} \\\n",
        "  --model=taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yxOT19QS0olH"
      },
      "source": [
        "Change the working directory context in this notebook to the TFX project directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6P-HljcU0olI",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/home/jupyter/mlops-on-gcp/workshops/ml-pipelines/tfx_template/my_pipeline_JK\n"
        }
      ],
      "source": [
        "%cd {PROJECT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1tEYUQxH0olO"
      },
      "source": [
        ">NOTE: You might also want to change to the `tfx_template/{PIPELINE_NAME}` directory in the JupyterLab file browser in the left nav by clicking into the directory once it is created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IzT2PFrN0olQ"
      },
      "source": [
        "## Step 3. Browse your copied source files\n",
        "\n",
        "The TFX template provides basic scaffold files to build a pipeline, including Python source code, sample data, and Jupyter Notebooks to analyse the output of the pipeline. The `taxi` template uses the same *Chicago Taxi* dataset and ML model as the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
        "\n",
        "Here is brief introduction to each of the Python files.\n",
        "-   `pipeline` - This directory contains the definition of the pipeline\n",
        "    -   `configs.py` — defines common constants for pipeline runners\n",
        "    -   `pipeline.py` — defines TFX components and a pipeline\n",
        "-   `models` - This directory contains ML model definitions.\n",
        "    -   `features.py`, `features_test.py` — defines features for the model\n",
        "    -   `preprocessing.py`, `preprocessing_test.py` — defines preprocessing\n",
        "        jobs using `tf::Transform`\n",
        "    -   `estimator` - This directory contains an Estimator based model.\n",
        "        -   `constants.py` — defines constants of the model\n",
        "        -   `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
        "    -   `keras` - This directory contains a Keras based model.\n",
        "        -   `constants.py` — defines constants of the model\n",
        "        -   `model.py`, `model_test.py` — defines DNN model using Keras\n",
        "        -   **Note: currently there's some issues with Keras model function. It's not recommended to use Keras model in this tutorial.**\n",
        "-   `beam_dag_runner.py`, `ai_platform_pipelines_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n",
        "\n",
        ">**In this tutorial we're going to use `ai_platform_pipelines_dag_runner.py` mainly.**\n",
        "\n",
        "List the files in the project directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YA9MFs7f0olR",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ai_platform_pipelines_dag_runner.py  kubeflow_dag_runner.py\nbeam_dag_runner.py\t\t     model_analysis.ipynb\ndata\t\t\t\t     models\ndata_validation.ipynb\t\t     pipeline\n__init__.py\n"
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ROwHAsDK0olT"
      },
      "source": [
        "You might notice that there are some files with `_test.py` in their name. These are unit tests for the pipeline, and it is recommended to add more unit tests as you implement your own pipelines.\n",
        "You can run unit tests by supplying the module name of test files via the `-m` flag. You can usually obtain a module name by deleting the `.py` extension and replacing `/` with `.`.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "M0cMdE2Z0olU",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Running tests under Python 3.7.8: /opt/conda/bin/python\n[ RUN      ] FeaturesTest.testNumberOfBucketFeatureBucketCount\nINFO:tensorflow:time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\nI0830 01:28:33.336468 140341126493568 test_util.py:1973] time(__main__.FeaturesTest.testNumberOfBucketFeatureBucketCount): 0.0s\n[       OK ] FeaturesTest.testNumberOfBucketFeatureBucketCount\n[ RUN      ] FeaturesTest.testTransformedNames\nINFO:tensorflow:time(__main__.FeaturesTest.testTransformedNames): 0.0s\nI0830 01:28:33.336899 140341126493568 test_util.py:1973] time(__main__.FeaturesTest.testTransformedNames): 0.0s\n[       OK ] FeaturesTest.testTransformedNames\n[ RUN      ] FeaturesTest.test_session\n[  SKIPPED ] FeaturesTest.test_session\n----------------------------------------------------------------------\nRan 3 tests in 0.001s\n\nOK (skipped=1)\nRunning tests under Python 3.7.8: /opt/conda/bin/python\n[ RUN      ] ModelTest.testBuildKerasModel\n2020-08-30 01:28:36.901027: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n2020-08-30 01:28:36.901598: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555bc5883d90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-08-30 01:28:36.901640: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-08-30 01:28:36.902064: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nI0830 01:28:36.987619 139891067524480 layer_utils.py:192] Model: \"functional_1\"\nI0830 01:28:36.987837 139891067524480 layer_utils.py:193] __________________________________________________________________________________________________\nI0830 01:28:36.987950 139891067524480 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \nI0830 01:28:36.988019 139891067524480 layer_utils.py:195] ==================================================================================================\nI0830 01:28:36.988266 139891067524480 layer_utils.py:190] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \nI0830 01:28:36.988358 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.988546 139891067524480 layer_utils.py:190] trip_miles_xf (InputLayer)      [(None,)]            0                                            \nI0830 01:28:36.988617 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.988785 139891067524480 layer_utils.py:190] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \nI0830 01:28:36.988856 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.989129 139891067524480 layer_utils.py:190] dense_features (DenseFeatures)  (None, 1)            0           pickup_latitude_xf[0][0]         \nI0830 01:28:36.989252 139891067524480 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \nI0830 01:28:36.989337 139891067524480 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \nI0830 01:28:36.989402 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.989765 139891067524480 layer_utils.py:190] dense (Dense)                   (None, 1)            2           dense_features[0][0]             \nI0830 01:28:36.989865 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.990094 139891067524480 layer_utils.py:190] dense_1 (Dense)                 (None, 1)            2           dense[0][0]                      \nI0830 01:28:36.990200 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.990453 139891067524480 layer_utils.py:190] dense_features_1 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \nI0830 01:28:36.990535 139891067524480 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \nI0830 01:28:36.990620 139891067524480 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \nI0830 01:28:36.990693 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.990838 139891067524480 layer_utils.py:190] concatenate (Concatenate)       (None, 35)           0           dense_1[0][0]                    \nI0830 01:28:36.990929 139891067524480 layer_utils.py:190]                                                                  dense_features_1[0][0]           \nI0830 01:28:36.990975 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.991303 139891067524480 layer_utils.py:190] dense_2 (Dense)                 (None, 1)            36          concatenate[0][0]                \nI0830 01:28:36.991438 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:36.991697 139891067524480 layer_utils.py:190] tf_op_layer_Squeeze (TensorFlow [(None,)]            0           dense_2[0][0]                    \nI0830 01:28:36.991805 139891067524480 layer_utils.py:257] ==================================================================================================\nI0830 01:28:36.992369 139891067524480 layer_utils.py:268] Total params: 40\nI0830 01:28:36.992469 139891067524480 layer_utils.py:269] Trainable params: 40\nI0830 01:28:36.992550 139891067524480 layer_utils.py:270] Non-trainable params: 0\nI0830 01:28:36.992606 139891067524480 layer_utils.py:271] __________________________________________________________________________________________________\nI0830 01:28:37.072632 139891067524480 layer_utils.py:192] Model: \"functional_3\"\nI0830 01:28:37.072812 139891067524480 layer_utils.py:193] __________________________________________________________________________________________________\nI0830 01:28:37.072899 139891067524480 layer_utils.py:190] Layer (type)                    Output Shape         Param #     Connected to                     \nI0830 01:28:37.072982 139891067524480 layer_utils.py:195] ==================================================================================================\nI0830 01:28:37.073235 139891067524480 layer_utils.py:190] pickup_latitude_xf (InputLayer) [(None,)]            0                                            \nI0830 01:28:37.073351 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.073537 139891067524480 layer_utils.py:190] trip_miles_xf (InputLayer)      [(None,)]            0                                            \nI0830 01:28:37.073621 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.073794 139891067524480 layer_utils.py:190] trip_start_hour_xf (InputLayer) [(None,)]            0                                            \nI0830 01:28:37.073886 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.074169 139891067524480 layer_utils.py:190] dense_features_2 (DenseFeatures (None, 1)            0           pickup_latitude_xf[0][0]         \nI0830 01:28:37.074275 139891067524480 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \nI0830 01:28:37.074329 139891067524480 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \nI0830 01:28:37.074374 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.074609 139891067524480 layer_utils.py:190] dense_3 (Dense)                 (None, 1)            2           dense_features_2[0][0]           \nI0830 01:28:37.074673 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.074845 139891067524480 layer_utils.py:190] dense_features_3 (DenseFeatures (None, 34)           0           pickup_latitude_xf[0][0]         \nI0830 01:28:37.074906 139891067524480 layer_utils.py:190]                                                                  trip_miles_xf[0][0]              \nI0830 01:28:37.074956 139891067524480 layer_utils.py:190]                                                                  trip_start_hour_xf[0][0]         \nI0830 01:28:37.074999 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.075103 139891067524480 layer_utils.py:190] concatenate_1 (Concatenate)     (None, 35)           0           dense_3[0][0]                    \nI0830 01:28:37.075156 139891067524480 layer_utils.py:190]                                                                  dense_features_3[0][0]           \nI0830 01:28:37.075199 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.075343 139891067524480 layer_utils.py:190] dense_4 (Dense)                 (None, 1)            36          concatenate_1[0][0]              \nI0830 01:28:37.075401 139891067524480 layer_utils.py:259] __________________________________________________________________________________________________\nI0830 01:28:37.075554 139891067524480 layer_utils.py:190] tf_op_layer_Squeeze_1 (TensorFl [(None,)]            0           dense_4[0][0]                    \nI0830 01:28:37.075617 139891067524480 layer_utils.py:257] ==================================================================================================\nI0830 01:28:37.075954 139891067524480 layer_utils.py:268] Total params: 38\nI0830 01:28:37.076026 139891067524480 layer_utils.py:269] Trainable params: 38\nI0830 01:28:37.076069 139891067524480 layer_utils.py:270] Non-trainable params: 0\nI0830 01:28:37.076112 139891067524480 layer_utils.py:271] __________________________________________________________________________________________________\nINFO:tensorflow:time(__main__.ModelTest.testBuildKerasModel): 0.21s\nI0830 01:28:37.076631 139891067524480 test_util.py:1973] time(__main__.ModelTest.testBuildKerasModel): 0.21s\n[       OK ] ModelTest.testBuildKerasModel\n[ RUN      ] ModelTest.test_session\n[  SKIPPED ] ModelTest.test_session\n----------------------------------------------------------------------\nRan 2 tests in 0.209s\n\nOK (skipped=1)\n"
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m models.features_test\n",
        "!{sys.executable} -m models.keras.model_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tO9Jhplo0olX"
      },
      "source": [
        "## Step 4. Run your first TFX pipeline\n",
        "\n",
        "Components in the TFX pipeline will generate outputs for each run as [ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere. Currently, AI Platform Pipelines supports Google Cloud Storage (GCS).\n",
        "\n",
        "To run this pipeline you **MUST** edit `pipeline/configs.py` under the generated `tfx/{PIPELINE_NAME}` directory to set your GCS bucket name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hbhA7kWCIJHu"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "'mlops-dev-workspace'"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# edit 'pipeline/configs.py' to set `GCS_BUCKET_NAME` to the `BUCKET_NAME` value you set earlier:\n",
        "BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OWcWmGfw0old"
      },
      "source": [
        "**Double-click to change your directory to `pipeline` and double-click again to open `configs.py`**. Set `GCS_BUCKET_NAME` to the name of your `BUCKET_NAME` GCS bucket without the `gs://` or trailing `/`.\n",
        "\n",
        "**Note:** The auto-generated bucket name won't work for managed CAIP pipelines. Please make sure you have specified the value of `GCS_BUCKET_NAME`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EIBkJ4bYjxVK",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "ERROR:absl:Set your GCS_BUCKET_NAME in the `pipeline/configs.py` file.\n"
        }
      ],
      "source": [
        "# Let's make sure you have set YOUR bucket name\n",
        "# DO NOT edit following code. You should set your bucket name in the `pipeline/configs.py` file.\n",
        "from absl import logging\n",
        "try:\n",
        "    from pipeline import configs\n",
        "    import imp; imp.reload(configs)\n",
        "    if configs.GCS_BUCKET_NAME == 'mlops-dev-workspace':\n",
        "        logging.error('Set your GCS_BUCKET_NAME in the `pipeline/configs.py` file.')\n",
        "except ImportError:\n",
        "    logging.error('Please make sure that `pipeline/configs.py` file exists.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MLPA7BobCLTM"
      },
      "source": [
        "After making sure the GCS bucket is set, let's upload our sample data there so that we can use it in our pipeline later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iEpOrhqECOUI",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Copying file://data/data.csv [Content-Type=text/csv]...\n\nOperation completed over 1 objects/1.9 MiB.                                      \n"
        }
      ],
      "source": [
        "!gsutil cp data/data.csv gs://{configs.GCS_BUCKET_NAME}/tfx-template/data/data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wc54hDZu0ole"
      },
      "source": [
        "Let's create a TFX pipeline using the `tfx caipp pipeline create` command.\n",
        "\n",
        "We need a container image which will be used to run our pipeline. We'll use `skaffold` to build the image for us. \n",
        "The build process may take 5-10 minutes the first time, but will be much quicker for subsequent builds.\n",
        "\n",
        "> Note: if you get a permissions error running the build, try first running\n",
        "```sh\n",
        "gcloud auth login\n",
        "```\n",
        "in the notebook terminal window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kOU7zQof0olf",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CLI\nCloud AI Platform Pipelines\nCreating pipeline\nReading build spec from build.yaml\nNo local setup.py, copying the directory and configuring the PYTHONPATH.\n[Skaffold] invalid skaffold config: invalid imageName 'gcr.io/mlops-dev-env/tfx-template-JK': repository name must be lowercase\nNo container image is built.\nTraceback (most recent call last):\n  File \"/opt/conda/bin/tfx\", line 8, in <module>\n    sys.exit(cli_group())\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/click/decorators.py\", line 73, in new_func\n    return ctx.invoke(f, obj, *args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/ai_platform_pipelines/commands/pipeline.py\", line 86, in create_pipeline\n    ctx.flags_dict).create_pipeline()\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/ai_platform_pipelines/handler/ai_platform_pipelines_handler.py\", line 101, in create_pipeline\n    skaffold_cmd)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/ai_platform_pipelines/handler/ai_platform_pipelines_handler.py\", line 354, in _build_pipeline_image\n    skaffold_cmd=skaffold_cmd).build()\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/container_builder/builder.py\", line 92, in build\n    image_sha = skaffold_cli.build(self._buildspec)\n  File \"/home/jupyter/.local/lib/python3.7/site-packages/tfx/tools/cli/container_builder/skaffold_cli.py\", line 61, in build\n    spec.filename))\nRuntimeError: skaffold failed to build an image with build.yaml.\n\u001b[0m"
        }
      ],
      "source": [
        "!tfx caipp pipeline create  \\\n",
        "--pipeline-path=ai_platform_pipelines_dag_runner.py \\\n",
        "--build-base-image={BASE_IMAGE} \\\n",
        "--build-target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QmA6___Y0olh"
      },
      "source": [
        "During the process of creating a pipeline, `Dockerfile` and `build.yaml` files will be generated to build a Docker image. For your own projects, you'll want to add these generated files to your source control system (e.g., git) along with other source files.\n",
        "\n",
        "Now start an execution run with the newly created pipeline using the `tfx caipp run create` command. We need to tell the CLI to use the image we just built.\n",
        "\n",
        "Note that this command is using the API key you set earlier, so that the TFX CLI is authenticated to trigger AI Platform Managed Pipeline executions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cKSjVVsa0oli"
      },
      "outputs": [],
      "source": [
        "!tfx caipp run create --pipeline-name={PIPELINE_NAME} \\\n",
        "--project-id={PROJECT_ID} \\\n",
        "--api-key={API_KEY} \\\n",
        "--target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vyCvBEtFcbK9"
      },
      "source": [
        "Visit the AI Platform Pipelines [UI](https://console.cloud.google.com/ai-platform/pipelines/runs) in the Cloud Console to explore some information about the pipeline run you just triggered. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bYH8Y2KB0olm"
      },
      "source": [
        "## Step 5. Add components for data validation.\n",
        "\n",
        "In this step, you will add components for data validation, including `StatisticsGen`, `SchemaGen`, and `ExampleValidator`. If you are interested in data validation, please see [Get started with Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started).\n",
        "\n",
        ">**Double-click to open `pipeline.py`**. Find and uncomment the 3 lines which add `StatisticsGen`, `SchemaGen`, and `ExampleValidator` to the pipeline. (Tip: search for comments containing `TODO(step 5):`).  Make sure to save `pipeline.py` after you edit it.\n",
        "\n",
        "You now need to update the existing pipeline with modified pipeline definition. Use the `tfx caipp pipeline update` command to update your pipeline, followed by the `tfx caipp run create` command to create a new execution run of your updated pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VE-Pqvto0olm"
      },
      "outputs": [],
      "source": [
        "# Update the pipeline\n",
        "!tfx caipp pipeline update \\\n",
        "--pipeline-path=ai_platform_pipelines_dag_runner.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bx0s3EBoIJH8"
      },
      "source": [
        "You can run the pipeline the same way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FSd9dfMGIJH-"
      },
      "outputs": [],
      "source": [
        "!tfx caipp run create --pipeline-name={PIPELINE_NAME} \\\n",
        "--project-id={PROJECT_ID} \\\n",
        "--api-key={API_KEY} \\\n",
        "--target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8q1ZYEHX0olo"
      },
      "source": [
        "### Check pipeline outputs\n",
        "\n",
        "You can go to the managed CAIP pipelines [UI](https://console.cloud.google.com/ai-platform/pipelines/runs) and find your pipeline execution by name.\n",
        "By clicking the pipeline name you can explore the DAG UI associated with the execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dWMBXU510olp"
      },
      "source": [
        "## Step 6. Add components for training.\n",
        "\n",
        "In this step, you will add components for training and model validation including `Transform`, `Trainer`, 'ResolverNode', `Evaluator`, and `Pusher`.\n",
        "\n",
        ">**Double-click to open the `pipeline.py` file**. Find and uncomment the 5 lines which add `Transform`, `Trainer`, `ResolverNode`, `Evaluator` and `Pusher` to the pipeline. (Tip: search for `TODO(step 6):`)\n",
        "\n",
        "As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using `tfx caipp pipeline update`, and create an execution run using `tfx caipp run create`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VQDNitkH0olq"
      },
      "outputs": [],
      "source": [
        "!tfx caipp pipeline update \\\n",
        "--pipeline-path=ai_platform_pipelines_dag_runner.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OdZT9iYeIJID"
      },
      "outputs": [],
      "source": [
        "!tfx caipp run create --pipeline-name={PIPELINE_NAME} \\\n",
        "--project-id={PROJECT_ID} \\\n",
        "--api-key={API_KEY} \\\n",
        "--target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ksWfVQUnMYCX"
      },
      "source": [
        "When this execution run finishes successfully, you have now created and run your first TFX pipeline in AI Platform Pipelines!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nkF7klWi0ols"
      },
      "source": [
        "## Step 7 (*Optional*) Try BigQueryExampleGen\n",
        "\n",
        "[BigQuery](https://cloud.google.com/bigquery) is a serverless, highly scalable, and cost-effective cloud data warehouse. BigQuery can be used as a source for training examples in TFX. In this step, we will add `BigQueryExampleGen` to the pipeline.\n",
        "\n",
        "**Double-click to open `pipeline.py`**. Comment out `CsvExampleGen` and uncomment the line which creates an instance of `BigQueryExampleGen`. You also need to uncomment the `query` argument of the `create_pipeline` function.\n",
        "\n",
        "We need to specify which GCP project to use for BigQuery, and this is done by setting `--project` in `beam_pipeline_args` when creating a pipeline.\n",
        "\n",
        "1. **Double-click to open `configs.py`**. Uncomment and set the definitions of `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_REGION`.  Then uncomment `BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` and `BIG_QUERY_QUERY`. Replace the project id and the region value in this file with the correct values for your GCP project. (The script tries to automatically grab the `GOOGLE_CLOUD_PROJECT` value, so you may not need to explicitly set it).\n",
        "\n",
        "1. **Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline— `my_pipeline_{USER}`, if you didn't change it. \n",
        "\n",
        "1. **Double-click to open `ai_platform_pipelines_dag_runner.py`**. Uncomment two arguments, `query` and `beam_pipeline_args`, for the `create_pipeline` function.\n",
        "\n",
        "Now the pipeline is ready to use BigQuery as an example source. Update the pipeline as before and create a new execution run as we did above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1sD3NxB60olt"
      },
      "outputs": [],
      "source": [
        "!tfx caipp pipeline update \\\n",
        "--pipeline-path=ai_platform_pipelines_dag_runner.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I8KJAG1lIJII"
      },
      "outputs": [],
      "source": [
        "!tfx caipp run create --pipeline-name={PIPELINE_NAME} \\\n",
        "--project-id={PROJECT_ID} \\\n",
        "--api-key={API_KEY} \\\n",
        "--target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LpA2R6Lu0olv"
      },
      "source": [
        "## Step 8 (*Optional*) Try Dataflow with AI Platform Pipelines\n",
        "\n",
        "Several [TFX Components use Apache Beam](https://www.tensorflow.org/tfx/guide/beam) to implement data-parallel pipelines, and it means that you can distribute data processing workloads using [Google Cloud Dataflow](https://cloud.google.com/dataflow/). In this step, we will set the Kubeflow orchestrator to use dataflow as the data processing back-end for Apache Beam. We assume you've made the edits to run Step 8.1 above.\n",
        "\n",
        "1. **Double-click `pipeline` to change directory, and double-click to open `configs.py`**. In Step 8.1 above, you should have already uncommented and set the definitions of `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_REGION`. Also uncomment `DATAFLOW_BEAM_PIPELINE_ARGS`.\n",
        "\n",
        "1. **Double-click to open `pipeline.py`**. Change the value of `enable_cache` to `False`.\n",
        "\n",
        "1. **Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline, which is `my_pipeline_{USER}`, if you didn't change it.\n",
        "\n",
        "1. **Double-click to open `ai_platform_pipelines_dag_runner.py`**. Change `BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` to `DATAFLOW_BEAM_PIPELINE_ARGS`)\n",
        "\n",
        "Note that we deliberately disabled caching. Because we have already run the pipeline successfully, we will get cached execution results for all components if the cache is enabled.\n",
        "\n",
        "Now the pipeline is ready to use Dataflow. Update the pipeline and create a new execution run as we did above.\n",
        "\n",
        "**Note:** In this template notebook Dataflow can only work with the BigQueryExampleGen because CsvExampleGen is using a input location inside the customized container image. In order to use the Dataflow with CsvExampleGen please make sure your input location is in GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "H3HVPcKi0olw"
      },
      "outputs": [],
      "source": [
        "!tfx caipp pipeline update \\\n",
        "--pipeline-path=ai_platform_pipelines_dag_runner.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bgKkzxbyIJIN"
      },
      "outputs": [],
      "source": [
        "!tfx caipp run create --pipeline-name={PIPELINE_NAME} \\\n",
        "--project-id={PROJECT_ID} \\\n",
        "--api-key={API_KEY} \\\n",
        "--target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_uxDY13N0oly"
      },
      "source": [
        "You can find your Dataflow jobs in [Dataflow in Cloud Console](http://console.cloud.google.com/dataflow).\n",
        "\n",
        "Please reset `enable_cache` to `True` to benefit from caching execution results. **Double-click to open `pipeline.py`**. Set the value of `enable_cache` to `True`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kJOmh1RY0olz"
      },
      "source": [
        "## Step 9. (*Optional*) Try Cloud AI Platform Training and Prediction with AI Platform Pipelines\n",
        "\n",
        "TFX interoperates with several managed GCP services, such as [Cloud AI Platform for Training and Prediction](https://cloud.google.com/ai-platform/). You can set your `Trainer` component to use Cloud AI Platform Training, a managed service for training ML models. Moreover, when your model is built and ready to be served, you can *push* your model to Cloud AI Platform Prediction for serving. In this step, we will set our `Trainer` and `Pusher` component to use Cloud AI Platform services.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "q-M1linjIJIQ"
      },
      "outputs": [],
      "source": [
        "# get a reminder of this value set earlier\n",
        "CUSTOM_TFX_IMAGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q6B-JtQ4IJIS"
      },
      "source": [
        "**Double-click `pipeline` to change the directory, and double-click to open `configs.py`**. Uncomment and set the definition of `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_REGION` as necessary. Then uncomment `GCP_AI_PLATFORM_TRAINING_ARGS` and `GCP_AI_PLATFORM_SERVING_ARGS`.    \n",
        "\n",
        "We will use our custom-built container image to train a model in Cloud AI Platform Training, so set `masterConfig.imageUri` in `GCP_AI_PLATFORM_TRAINING_ARGS` to the same value as `CUSTOM_TFX_IMAGE` defined above.\n",
        "\n",
        "**Change directory one level up, and double-click to open `ai_platform_pipelines_dag_runner.py`**. Uncomment `ai_platform_training_args` and `ai_platform_serving_args`.\n",
        "\n",
        "Update the pipeline and create a new execution run as we did in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yxOjhBmG0ol0"
      },
      "outputs": [],
      "source": [
        "!tfx caipp pipeline update \\\n",
        "--pipeline-path=ai_platform_pipelines_dag_runner.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "8KdWE6zzIJIV"
      },
      "outputs": [],
      "source": [
        "!tfx caipp run create --pipeline-name={PIPELINE_NAME} \\\n",
        "--project-id={PROJECT_ID} \\\n",
        "--api-key={API_KEY} \\\n",
        "--target-image={CUSTOM_TFX_IMAGE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BkoIMUfj0ol2"
      },
      "source": [
        "You can find your training jobs in [Cloud AI Platform Jobs](https://console.cloud.google.com/ai-platform/jobs). If your pipeline completed successfully, you can find your model in [Cloud AI Platform Models](https://console.cloud.google.com/ai-platform/models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Sb_YCUHIJIX"
      },
      "source": [
        "## Cleanup\n",
        "\n",
        "If you like, you can do some cleanup to avoid storage costs.\n",
        "\n",
        "To remove the files from your GCS bucket, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Vvs4QcDiIJIY"
      },
      "outputs": [],
      "source": [
        "!gsutil rm 'gs://{BUCKET_NAME}/**'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aMoP7yXWIJIa"
      },
      "source": [
        "You can remove your GCR container images by visiting the [Container Registry](https://console.cloud.google.com/gcr/) panel in the Cloud Console.  Click on an image name to list and remove any of its versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qfLF-azpIJIb"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook showed examples of how to how to run a TFX Templates pipeline on Managed Pipelines.\n",
        "\n",
        "You can also explore notebooks that show how to specify TFX pipelines using prebuilt components; and how to build custom functions and components. See the EAP guide for the links.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "caipp_eap_tfx_template.ipynb",
      "provenance": [
        {
          "file_id": "17XdWONvY6S1krXZdSrGZKKv98wKd9XKw",
          "timestamp": 1592238930390
        }
      ]
    },
    "environment": {
      "name": "tf22-gpu.2-2.m47",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf22-gpu.2-2:m47"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}